services:
  llm-service:
    container_name: llm-service
    build: .
    ports:
      - "5001:5000"
    # platform: linux/arm64
    environment:
      - MODEL_NAME=meta-llama/Llama-3.2-3B-Instruct
      - DEVICE=cuda
      - DTYPE=int8 # float32, float16, int8, int4
      - GENERATION_PARAMETERS={"max_new_tokens":2048,"temperature":0.5,"repetition_penalty":1.1}
      - HUGGINGFACE_TOKEN=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
      - PYTHONUNBUFFERED=1
    volumes:
      - llm-model-cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  llm-model-cache: {}
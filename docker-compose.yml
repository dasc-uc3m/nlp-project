services:
  llm-service:
    container_name: llm-service
    build: .
    ports:
      - "5001:5000"
    # platform: linux/arm64
    environment:
      - MODEL_NAME=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
      - DEVICE=cuda
      - GENERATION_PARAMETERS={"max_new_tokens":2048,"temperature":0.5,"repetition_penalty":1.1,"length_penalty":0.8}
      - HUGGINGFACE_TOKEN=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
      - PYTHONUNBUFFERED=1
    volumes:
      - llm-model-cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  llm-model-cache: {}
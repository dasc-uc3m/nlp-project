services:
  llm-service:
    container_name: llm-service
    build: .
    ports:
      - "5001:5000"
    # platform: linux/arm64
    environment:
      - MODEL_NAME=Qwen/Qwen2.5-0.5B-Instruct
      - DEVICE=cuda
      - MAX_TOKENS=512
      - TEMPERATURE=0.7
      - HUGGINGFACE_TOKEN=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
      - PYTHONUNBUFFERED=1
    volumes:
      - llm-model-cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  llm-model-cache: {}
services:
  llm-service:
    container_name: llm-service
    build: .
    ports:
      - "5001:5000"
    # platform: linux/arm64
    environment:
      - MODEL_NAME=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
      - DEVICE=cuda
      - MAX_TOKENS=-1
      - TEMPERATURE=0.7
      - HUGGINGFACE_TOKEN=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
      - PYTHONUNBUFFERED=1
    volumes:
      - llm-model-cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  llm-model-cache: {}